{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/NLP%20for%20predicting%20success.ipynb#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Libraries</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/NLP%20for%20predicting%20success.ipynb#Import-Data\" data-toc-modified-id=\"Import-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import Data</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/NLP%20for%20predicting%20success.ipynb#NLP-on-the-Storyline\" data-toc-modified-id=\"NLP-on-the-Storyline-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>NLP on the Storyline</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/NLP%20for%20predicting%20success.ipynb#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Data Preparation</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/NLP%20for%20predicting%20success.ipynb#Cleaning-the-storylines\" data-toc-modified-id=\"Cleaning-the-storylines-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Cleaning the storylines</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/NLP%20for%20predicting%20success.ipynb#Analysis-of-Success\" data-toc-modified-id=\"Analysis-of-Success-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Analysis of Success</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/NLP%20for%20predicting%20success.ipynb#Cross-Validation\" data-toc-modified-id=\"Cross-Validation-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Cross Validation</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/NLP%20for%20predicting%20success.ipynb#Analysis\" data-toc-modified-id=\"Analysis-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Analysis</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:15:36.789537Z",
     "start_time": "2018-04-26T00:15:36.266362Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T07:17:59.196465Z",
     "start_time": "2018-04-26T07:17:59.193175Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:25:06.936947Z",
     "start_time": "2018-04-26T00:25:06.510421Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T23:54:28.496301Z",
     "start_time": "2018-04-25T23:54:28.411576Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_ready.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T23:54:33.539521Z",
     "start_time": "2018-04-25T23:54:33.507042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actor_1_id</th>\n",
       "      <th>Actor_1_name</th>\n",
       "      <th>Actor_2_id</th>\n",
       "      <th>Actor_2_name</th>\n",
       "      <th>Actor_3_id</th>\n",
       "      <th>Actor_3_name</th>\n",
       "      <th>Country</th>\n",
       "      <th>Director_id</th>\n",
       "      <th>Director_name</th>\n",
       "      <th>Genres</th>\n",
       "      <th>...</th>\n",
       "      <th>isWestern</th>\n",
       "      <th>Duration_movie_final</th>\n",
       "      <th>Bugdet_final</th>\n",
       "      <th>Gross_final</th>\n",
       "      <th>Opening Weekend final</th>\n",
       "      <th>Actor_1_like_final</th>\n",
       "      <th>Actor_2_like_final</th>\n",
       "      <th>Actor_3_like_final</th>\n",
       "      <th>Director_like_final</th>\n",
       "      <th>IMDb_rating_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nm0941777</td>\n",
       "      <td>Sam Worthington</td>\n",
       "      <td>nm0757855</td>\n",
       "      <td>Zoe Saldana</td>\n",
       "      <td>nm0000244</td>\n",
       "      <td>Sigourney Weaver</td>\n",
       "      <td>UK</td>\n",
       "      <td>nm0000116</td>\n",
       "      <td>\\nJames Cameron</td>\n",
       "      <td>[' Action', ' Adventure', ' Fantasy', ' Sci-Fi']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>2.787965e+09</td>\n",
       "      <td>77025481.0</td>\n",
       "      <td>5300.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nm3766090</td>\n",
       "      <td>Doug Walker</td>\n",
       "      <td>nm3597657</td>\n",
       "      <td>Rob Walker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nm3766090</td>\n",
       "      <td>\\nDoug Walker</td>\n",
       "      <td>[' Documentary']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nm0000136</td>\n",
       "      <td>Johnny Depp</td>\n",
       "      <td>nm0089217</td>\n",
       "      <td>Orlando Bloom</td>\n",
       "      <td>nm0461136</td>\n",
       "      <td>Keira Knightley</td>\n",
       "      <td>USA</td>\n",
       "      <td>nm0893659</td>\n",
       "      <td>\\nGore Verbinski</td>\n",
       "      <td>[' Action', ' Adventure', ' Fantasy']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>300000000.0</td>\n",
       "      <td>9.634204e+08</td>\n",
       "      <td>139802190.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>5100.0</td>\n",
       "      <td>7700.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nm0185819</td>\n",
       "      <td>Daniel Craig</td>\n",
       "      <td>nm0910607</td>\n",
       "      <td>Christoph Waltz</td>\n",
       "      <td>nm2244205</td>\n",
       "      <td>LÃ©a Seydoux</td>\n",
       "      <td>UK</td>\n",
       "      <td>nm0005222</td>\n",
       "      <td>\\nSam Mendes</td>\n",
       "      <td>[' Action', ' Adventure', ' Thriller']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>245000000.0</td>\n",
       "      <td>8.806742e+08</td>\n",
       "      <td>70403148.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nm0000288</td>\n",
       "      <td>Christian Bale</td>\n",
       "      <td>nm0362766</td>\n",
       "      <td>Tom Hardy</td>\n",
       "      <td>nm0004266</td>\n",
       "      <td>Anne Hathaway</td>\n",
       "      <td>UK</td>\n",
       "      <td>nm0634240</td>\n",
       "      <td>\\nChristopher Nolan</td>\n",
       "      <td>[' Action', ' Thriller']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>250000000.0</td>\n",
       "      <td>1.084939e+09</td>\n",
       "      <td>160887295.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Actor_1_id     Actor_1_name Actor_2_id     Actor_2_name Actor_3_id  \\\n",
       "0  nm0941777  Sam Worthington  nm0757855      Zoe Saldana  nm0000244   \n",
       "1  nm3766090      Doug Walker  nm3597657       Rob Walker        NaN   \n",
       "2  nm0000136      Johnny Depp  nm0089217    Orlando Bloom  nm0461136   \n",
       "3  nm0185819     Daniel Craig  nm0910607  Christoph Waltz  nm2244205   \n",
       "4  nm0000288   Christian Bale  nm0362766        Tom Hardy  nm0004266   \n",
       "\n",
       "       Actor_3_name Country Director_id         Director_name  \\\n",
       "0  Sigourney Weaver      UK   nm0000116      \\nJames Cameron    \n",
       "1               NaN     NaN   nm3766090        \\nDoug Walker    \n",
       "2   Keira Knightley     USA   nm0893659     \\nGore Verbinski    \n",
       "3       LÃ©a Seydoux      UK   nm0005222         \\nSam Mendes    \n",
       "4     Anne Hathaway      UK   nm0634240  \\nChristopher Nolan    \n",
       "\n",
       "                                             Genres        ...          \\\n",
       "0  [' Action', ' Adventure', ' Fantasy', ' Sci-Fi']        ...           \n",
       "1                                  [' Documentary']        ...           \n",
       "2             [' Action', ' Adventure', ' Fantasy']        ...           \n",
       "3            [' Action', ' Adventure', ' Thriller']        ...           \n",
       "4                          [' Action', ' Thriller']        ...           \n",
       "\n",
       "  isWestern Duration_movie_final Bugdet_final   Gross_final  \\\n",
       "0         0                162.0  237000000.0  2.787965e+09   \n",
       "1         0                  NaN          NaN           NaN   \n",
       "2         0                169.0  300000000.0  9.634204e+08   \n",
       "3         0                148.0  245000000.0  8.806742e+08   \n",
       "4         0                164.0  250000000.0  1.084939e+09   \n",
       "\n",
       "  Opening Weekend final  Actor_1_like_final  Actor_2_like_final  \\\n",
       "0            77025481.0              5300.0                 6.0   \n",
       "1                   NaN                 NaN                12.0   \n",
       "2           139802190.0             40000.0              5100.0   \n",
       "3            70403148.0              8500.0             11000.0   \n",
       "4           160887295.0                 3.0                 3.0   \n",
       "\n",
       "   Actor_3_like_final  Director_like_final  IMDb_rating_final  \n",
       "0                 2.0               7000.0                7.8  \n",
       "1                 NaN                  NaN                6.1  \n",
       "2              7700.0                582.0                7.1  \n",
       "3              1800.0                  0.0                6.8  \n",
       "4             11000.0                  2.0                8.4  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T23:54:45.776739Z",
     "start_time": "2018-04-25T23:54:45.763308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor_1_id                 25\n",
       "Actor_1_name               25\n",
       "Actor_2_id                 38\n",
       "Actor_2_name               38\n",
       "Actor_3_id                 46\n",
       "Actor_3_name               46\n",
       "Country                    38\n",
       "Director_id               134\n",
       "Director_name             134\n",
       "Genres                      0\n",
       "IMDb_critics               53\n",
       "Keywords                    0\n",
       "Language                   46\n",
       "Movie_name                  0\n",
       "Storyline                  42\n",
       "Year                      156\n",
       "label_country              40\n",
       "label_language             48\n",
       "isAction                    0\n",
       "isAdventure                 0\n",
       "isAnimation                 0\n",
       "isBiography                 0\n",
       "isComedy                    0\n",
       "isCrime                     0\n",
       "isFantasy                   0\n",
       "isDocumentary               0\n",
       "isFamily                    0\n",
       "isDrama                     0\n",
       "isHistory                   0\n",
       "isHorror                    0\n",
       "isMusic                     0\n",
       "isMystery                   0\n",
       "isRomance                   0\n",
       "isSciFi                     0\n",
       "isThriller                  0\n",
       "isSport                     0\n",
       "isSuperhero                 0\n",
       "isWar                       0\n",
       "isWestern                   0\n",
       "Duration_movie_final       48\n",
       "Bugdet_final              551\n",
       "Gross_final              2795\n",
       "Opening Weekend final    1118\n",
       "Actor_1_like_final        535\n",
       "Actor_2_like_final        587\n",
       "Actor_3_like_final        622\n",
       "Director_like_final       336\n",
       "IMDb_rating_final          53\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP on the Storyline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:11:18.351528Z",
     "start_time": "2018-04-26T00:11:18.345530Z"
    }
   },
   "outputs": [],
   "source": [
    "index_gross = pd.isnull(data['Gross_final'])\n",
    "X = data['Storyline'][~index_gross]\n",
    "y = data[['Gross_final','Bugdet_final']][~index_gross]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:11:40.013949Z",
     "start_time": "2018-04-26T00:11:40.010573Z"
    }
   },
   "outputs": [],
   "source": [
    "y_success  = np.where(y['Gross_final'] >= 2 * y['Bugdet_final'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:12:10.102220Z",
     "start_time": "2018-04-26T00:12:10.098003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_success[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:24:31.848026Z",
     "start_time": "2018-04-26T00:24:31.842706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of the reviews are:\n",
      "578.523052464229\n"
     ]
    }
   ],
   "source": [
    "lengths = X.apply(len)\n",
    "\n",
    "print('Average character length of the reviews are:')\n",
    "print (np.mean(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the storylines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:29:30.975285Z",
     "start_time": "2018-04-26T00:29:30.970963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhen his brother is killed in a robbery, paraplegic Marine Jake Sully decides to take his place in a mission on the distant world of Pandora. There he learns of greedy corporate figurehead Parker Selfridge\\'s intentions of driving off the native humanoid \"Na\\'vi\" in order to mine for the precious material scattered throughout their rich woodland. In exchange for the spinal surgery that will fix his legs, Jake gathers intel for the cooperating military unit spearheaded by gung-ho Colonel Quaritch, while simultaneously attempting to infiltrate the Na\\'vi people with the use of an \"avatar\" identity. While Jake begins to bond with the native tribe and quickly falls in love with the beautiful alien Neytiri, the restless Colonel moves forward with his ruthless extermination tactics, forcing the soldier to take a stand - and fight back in an epic battle for the fate of Pandora.                '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:56:38.939693Z",
     "start_time": "2018-04-26T00:56:38.921352Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_cleaner(review):\n",
    "    '''\n",
    "    Clean and preprocess a review.\n",
    "    \n",
    "    1. Remove HTML tags\n",
    "    2. Use regex to remove all special characters (only keep letters)\n",
    "    3. Make strings to lower case and tokenize / word split reviews\n",
    "    4. Remove English stopwords\n",
    "    5. Rejoin to one string\n",
    "    '''\n",
    "    \n",
    "    #1. Remove HTML tags\n",
    "    review = bs.BeautifulSoup(review).text\n",
    "    \n",
    "    #2. Use regex to find emoticons\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', review)\n",
    "    \n",
    "    #3. Remove punctuation\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \",review)\n",
    "    \n",
    "    #4. Tokenize into words (all lower case)\n",
    "    review = review.lower().split()\n",
    "    \n",
    "    #5. Remove stopwords\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    review = [w for w in review if not w in eng_stopwords]\n",
    "    \n",
    "    #6. Join the review to one sentence\n",
    "    review = ' '.join(review+emoticons)\n",
    "    # add emoticons to the end\n",
    "\n",
    "    return(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:56:39.566912Z",
     "start_time": "2018-04-26T00:56:39.560203Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Romain/.virtualenvs/romainvenv/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'elizabeth captain barbossa rescue captain jack sparrow land dead must face foes davy jones lord cutler beckett beckett control jones heart forms dark alliance order rule seas wipe last pirates jack barbossa elizabeth tia dalma crew must call pirate lords four corners globe including infamous sao feng gathering pirate lords want release goddess calypso davy jones damned lover trap sent fear pirate lords must combine pieces bound ritual undo release hopes help fight pirates stand together make final stand freedom beckett jones norrington flying dutchman entire east india trading company'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_cleaner(X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:56:41.954446Z",
     "start_time": "2018-04-26T00:56:40.200701Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Romain/.virtualenvs/romainvenv/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "num_story = len(X)\n",
    "\n",
    "story_clean_original = []\n",
    "\n",
    "for story in X:\n",
    "    story_clean_original.append(review_cleaner(story))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:56:55.593489Z",
     "start_time": "2018-04-26T00:56:52.375847Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Porter stemming on the results in review_clean_original\n",
    "\n",
    "story_clean_ps = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "for i in range(0,num_story): \n",
    "    ps_stems = []\n",
    "    for w in story_clean_original[i].split():\n",
    "        if w == 'oed':\n",
    "            continue\n",
    "        ps_stems.append(ps.stem(w))\n",
    "    \n",
    "    story_clean_ps.append(' '.join(ps_stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:56:55.602563Z",
     "start_time": "2018-04-26T00:56:55.595237Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:57:04.899847Z",
     "start_time": "2018-04-26T00:56:55.604556Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lemmatizer\n",
    "\n",
    "story_clean_wnl = []\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for i in range(0,num_story):\n",
    "    wnl_stems = []\n",
    "    token_tag = pos_tag(story_clean_original[i].split())\n",
    "    for pair in token_tag:\n",
    "        res = wnl.lemmatize(pair[0],pos=get_wordnet_pos(pair[1]))\n",
    "        wnl_stems.append(res)\n",
    "\n",
    "    story_clean_wnl.append(' '.join(wnl_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:57:16.599948Z",
     "start_time": "2018-04-26T00:57:16.589485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size (1887,)\n",
      "test set size (471,)\n",
      "Proportion of success in train set 0.6444091149973503\n",
      "Proportion of success in test set 0.6496815286624203\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "test_size = int(X.shape[0]/5.0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_success, test_size = test_size, random_state = seed)\n",
    "print(\"train set size\", X_train.shape)\n",
    "print(\"test set size\", X_test.shape)\n",
    "print(\"Proportion of success in train set\", sum(y_train)/float(y_train.shape[0]))\n",
    "print(\"Proportion of success in test set\", sum(y_test)/float(y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:57:17.943324Z",
     "start_time": "2018-04-26T00:57:17.940062Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:57:18.684890Z",
     "start_time": "2018-04-26T00:57:18.477046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:57:18.938285Z",
     "start_time": "2018-04-26T00:57:18.931907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '007', '10', '100', '1000', '11', '12', '13', '14']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:57:19.590478Z",
     "start_time": "2018-04-26T00:57:19.382154Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bag = vectorizer.transform(X_train) #transform to a feature matrix\n",
    "test_bag = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:57:19.670691Z",
     "start_time": "2018-04-26T00:57:19.644986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1887, 5000)\n",
      "(471, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(train_bag.toarray().shape)\n",
    "print(test_bag.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:57:20.119887Z",
     "start_time": "2018-04-26T00:57:20.116311Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  sklearn.ensemble  import RandomForestClassifier\n",
    "\n",
    "## Initialize a Random Forest classifier with 50 trees\n",
    "# hyperparameter n_estimators always set in instantiation\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:57:21.103886Z",
     "start_time": "2018-04-26T00:57:20.612777Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = forest.fit(train_bag, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:57:21.159986Z",
     "start_time": "2018-04-26T00:57:21.105764Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_predictions = forest.predict(train_bag)\n",
    "valid_predictions = forest.predict(test_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:57:21.513309Z",
     "start_time": "2018-04-26T00:57:21.508205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9994700582935877"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_train,train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:57:21.827567Z",
     "start_time": "2018-04-26T00:57:21.823425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,valid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T06:45:03.456988Z",
     "start_time": "2018-04-26T06:45:03.450302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 25, 140],\n",
       "       [ 17, 289]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test,valid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T06:45:07.069425Z",
     "start_time": "2018-04-26T06:45:07.064764Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test,valid_predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T06:45:16.547095Z",
     "start_time": "2018-04-26T06:45:16.536418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.00024067 0.         ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "importances = forest.feature_importances_\n",
    "# returns relative importance of all features.\n",
    "# they are in the order of the columns\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T06:45:18.191209Z",
     "start_time": "2018-04-26T06:45:18.151675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "['and', 'the', 'to', 'brother', 'is', 'in', 'of', 'but', 'his', 'young']\n"
     ]
    }
   ],
   "source": [
    "# sort importance scores# sort i \n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "top_10 = indices[:10]\n",
    "\n",
    "# Get top ten features\n",
    "print([vectorizer.get_feature_names()[ind] for ind in top_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T07:17:34.172106Z",
     "start_time": "2018-04-26T07:17:34.169036Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'max_features': [500, 2000, 5000, 7000], \n",
    "          'n_estimators': [10, 25 , 50, 75],\n",
    "         'stop_words': [None, 'english']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T07:18:02.444824Z",
     "start_time": "2018-04-26T07:18:02.399574Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv(parameters):\n",
    "    best_acc_score = 0\n",
    "    index = 1\n",
    "    best_grid = None\n",
    "    \n",
    "    for g in ParameterGrid(parameters):\n",
    "        print('test number', index)\n",
    "        index = index + 1\n",
    "        max_features = g['max_features']\n",
    "        n_estimators = g['n_estimators']\n",
    "        stop_words = g['stop_words']\n",
    "\n",
    "        vectorizer = CountVectorizer(analyzer = \"word\", preprocessor = None, stop_words = stop_words, max_features = max_features)\n",
    "        vectorizer.fit(X_train)\n",
    "        train_bag = vectorizer.transform(X_train) #transform to a feature matrix\n",
    "        test_bag = vectorizer.transform(X_test)\n",
    "\n",
    "        forest = RandomForestClassifier(n_estimators = n_estimators)\n",
    "        forest = forest.fit(train_bag, y_train)\n",
    "    \n",
    "        train_predictions = forest.predict(train_bag)\n",
    "        valid_predictions = forest.predict(test_bag)\n",
    "\n",
    "        acc_score = metrics.accuracy_score(y_test,valid_predictions)\n",
    "        \n",
    "        print(\"Accuracy score: %0.5f\" % acc_score)\n",
    "        print(\"------------------------------------\")\n",
    "            \n",
    "            #keep the best AUC score\n",
    "        if acc_score > best_acc_score:\n",
    "            best_acc_score = acc_score\n",
    "            best_grid = g\n",
    "        print(\"Best intermediate AUC score: %0.5f\" % best_acc_score)\n",
    "        print(\" \")\n",
    "    print(\"Final Best AUC score: %0.5f\" % best_acc_score) \n",
    "    print(\"Grid:\", best_grid)\n",
    "    return best_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T07:18:28.255411Z",
     "start_time": "2018-04-26T07:18:02.724213Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test number 1\n",
      "Accuracy score: 0.56900\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.56900\n",
      " \n",
      "test number 2\n",
      "Accuracy score: 0.60722\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.60722\n",
      " \n",
      "test number 3\n",
      "Accuracy score: 0.63694\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.63694\n",
      " \n",
      "test number 4\n",
      "Accuracy score: 0.63482\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.63694\n",
      " \n",
      "test number 5\n",
      "Accuracy score: 0.65817\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.65817\n",
      " \n",
      "test number 6\n",
      "Accuracy score: 0.62208\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.65817\n",
      " \n",
      "test number 7\n",
      "Accuracy score: 0.66242\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.66242\n",
      " \n",
      "test number 8\n",
      "Accuracy score: 0.63907\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.66242\n",
      " \n",
      "test number 9\n",
      "Accuracy score: 0.63270\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.66242\n",
      " \n",
      "test number 10\n",
      "Accuracy score: 0.60722\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.66242\n",
      " \n",
      "test number 11\n",
      "Accuracy score: 0.66454\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.66454\n",
      " \n",
      "test number 12\n",
      "Accuracy score: 0.65605\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.66454\n",
      " \n",
      "test number 13\n",
      "Accuracy score: 0.67304\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 14\n",
      "Accuracy score: 0.62420\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 15\n",
      "Accuracy score: 0.65817\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 16\n",
      "Accuracy score: 0.64544\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 17\n",
      "Accuracy score: 0.59873\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 18\n",
      "Accuracy score: 0.58386\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 19\n",
      "Accuracy score: 0.64756\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 20\n",
      "Accuracy score: 0.63057\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 21\n",
      "Accuracy score: 0.63907\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 22\n",
      "Accuracy score: 0.64331\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 23\n",
      "Accuracy score: 0.65605\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 24\n",
      "Accuracy score: 0.64331\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 25\n",
      "Accuracy score: 0.60722\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 26\n",
      "Accuracy score: 0.57113\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 27\n",
      "Accuracy score: 0.66667\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 28\n",
      "Accuracy score: 0.64756\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 29\n",
      "Accuracy score: 0.65180\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 30\n",
      "Accuracy score: 0.65817\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 31\n",
      "Accuracy score: 0.66879\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "test number 32\n",
      "Accuracy score: 0.64119\n",
      "------------------------------------\n",
      "Best intermediate AUC score: 0.67304\n",
      " \n",
      "Final Best AUC score: 0.67304\n",
      "Grid: {'max_features': 2000, 'n_estimators': 50, 'stop_words': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_features': 2000, 'n_estimators': 50, 'stop_words': None}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T08:43:49.248381Z",
     "start_time": "2018-04-26T08:43:44.024815Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", preprocessor = None, stop_words = None, max_features = 2000)\n",
    "vectorizer.fit(X_train)\n",
    "train_bag = vectorizer.transform(X_train) #transform to a feature matrix\n",
    "test_bag = vectorizer.transform(X_test)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 500)\n",
    "forest = forest.fit(train_bag, y_train)\n",
    "    \n",
    "train_predictions = forest.predict(train_bag)\n",
    "valid_predictions = forest.predict(test_bag)\n",
    "\n",
    "acc_score = metrics.accuracy_score(y_test,valid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T08:43:49.257560Z",
     "start_time": "2018-04-26T08:43:49.250182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy on the test set is: 0.6602972399150743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 14, 151,   9, 297])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('the accuracy on the test set is:', acc_score)\n",
    "metrics.confusion_matrix(y_test,valid_predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T08:45:12.025919Z",
     "start_time": "2018-04-26T08:45:12.021972Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test,valid_predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T08:51:06.994633Z",
     "start_time": "2018-04-26T08:51:06.990338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the TPR rate is: 0.9705882352941176\n",
      "The FPR rate is: 0.9151515151515152\n"
     ]
    }
   ],
   "source": [
    "print('the TPR rate is:', tp / (fn + tp))\n",
    "print('The FPR rate is:', fp / (fp + tn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "romainvenv",
   "language": "python",
   "name": "romainvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
